{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Site and Parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://mykbostats.com/players/1\"\n",
    "apikey = \"b327144420a12f944bbdd4863c9fc0bffb28a1e3\" \n",
    "params = {\n",
    "    \"url\": url,\n",
    "    \"apikey\": apikey,\n",
    "    \"js_render\": \"true\",\n",
    "    \"premium_proxy\": \"true\",\n",
    "}\n",
    "response = requests.get(\"https://api.zenrows.com/v1/\", params=params)\n",
    "\n",
    "#print(response.content[:500])\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Name and Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = soup.select('title')[0].text.strip()\n",
    "player = t[:t.find(' - ')]\n",
    "\n",
    "teams = {'Doosan': 'Doosan Bears', \n",
    "         'Hanwha': 'Hanwha Eagles', \n",
    "         'Kia': 'Kia Tigers', \n",
    "         'Kiwoom': 'Kiwoom Heroes', \n",
    "         'KT': 'KT Wiz', \n",
    "         'LG': 'LG Twins', \n",
    "         'Lotte': 'Lotte Giants', \n",
    "         'NC': 'NC Dinos', \n",
    "         'Samsung': 'Samsung Lions', \n",
    "         'SSG': 'SSG Landers'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract table header information\n",
    "header = soup.select('thead th')\n",
    "for i in range(len(header)):\n",
    "  header[i] = header[i].text.strip()\n",
    "\n",
    "# remove 'Game Stats' data\n",
    "header = header[:header.index(\"Date\")]\n",
    "header = ['Name'] + header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Table Contents \n",
    "Write to contents to dataframe, then write to CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "import pandas as pd\n",
    "from unicodedata import numeric\n",
    "\n",
    "\n",
    "#to handle unicode data in table\n",
    "def uni_to_num (unicode):\n",
    "    if ('(' in unicode):\n",
    "       return unicode\n",
    "    elif len(unicode) == 0:\n",
    "        return None\n",
    "    elif len(unicode) == 1:\n",
    "        num = numeric(unicode)\n",
    "    elif unicode[-1].isdigit():\n",
    "        # normal number, ending in [0-9]\n",
    "        num = float(unicode)\n",
    "    else:\n",
    "        # Assume the last character is a vulgar fraction\n",
    "        num = float(unicode[:-1]) + numeric(unicode[-1])\n",
    "    return num\n",
    "\n",
    "\n",
    "#data to be inserted into dataframe later\n",
    "temp = []\n",
    "\n",
    "# Parse row data and add to temp\n",
    "rows = soup.select('tbody tr')\n",
    "\n",
    "for r in rows:\n",
    "  if (r.select_one('.left').text.strip() == 'Career'):\n",
    "    break\n",
    "\n",
    "  t = [player, r.select_one('.left').text.strip(), r.select_one('nobr').text.strip()]  \n",
    "  for i in r.select('td')[2:]:\n",
    "    t.append(uni_to_num(i.text.strip()))\n",
    "  temp.append(t)\n",
    "\n",
    "\n",
    "# init dataframe\n",
    "data = pd.DataFrame(data = temp, columns = header)\n",
    "\n",
    "# write to CSV\n",
    "data.to_csv('data.csv')\n",
    "\n",
    "# read CSV\n",
    "#df = pd.read_csv('.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 134\u001b[0m\n\u001b[0;32m    131\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://mykbostats.com/players/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m    132\u001b[0m soup \u001b[38;5;241m=\u001b[39m get_website(url)\n\u001b[1;32m--> 134\u001b[0m player \u001b[38;5;241m=\u001b[39m \u001b[43mget_player\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m header \u001b[38;5;241m=\u001b[39m get_header(soup)\n\u001b[0;32m    136\u001b[0m rows \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbody tr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 73\u001b[0m, in \u001b[0;36mget_player\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_player\u001b[39m (soup):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    Given BeautifulSoup object with HTML code, return tuple with player name and role (pitcher/batter) \u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     75\u001b[0m     name \u001b[38;5;241m=\u001b[39m t[:t\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m KBO\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPitching\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m t):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#%pip install ...\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import numeric\n",
    "from time import sleep\n",
    "\n",
    "def uni_to_num (unicode):\n",
    "    '''\n",
    "    Given string of unicode, convert into numerics.\n",
    "    '''\n",
    "    \n",
    "    if ('(' in unicode):\n",
    "       return unicode\n",
    "    elif len(unicode) == 0:\n",
    "        return None\n",
    "    elif len(unicode) == 1:\n",
    "        num = numeric(unicode)\n",
    "    elif unicode[-1].isdigit():\n",
    "        # normal number, ending in [0-9]\n",
    "        num = float(unicode)\n",
    "    else:\n",
    "        # Assume the last character is a vulgar fraction\n",
    "        num = float(unicode[:-1]) + numeric(unicode[-1])\n",
    "\n",
    "    return num\n",
    "\n",
    "\n",
    "\n",
    "def get_row_data (rows, player):\n",
    "    '''\n",
    "    Given HTML code of table with data and tuple containing player name and role, extract data and return as dataframe.\n",
    "    '''\n",
    "    \n",
    "    rows = soup.select('tbody tr')\n",
    "\n",
    "    temp = []\n",
    "    for r in rows:\n",
    "        # break loop to exclude data from Career onwards\n",
    "        if (r.select_one('.left').text.strip() == 'Career'):\n",
    "            break\n",
    "\n",
    "        t = [player[0], r.select_one('.left').text.strip(), r.select_one('nobr').text.strip()]  \n",
    "        for i in r.select('td')[2:]:\n",
    "            t.append(uni_to_num(i.text.strip()))\n",
    "        temp.append(t)\n",
    "\n",
    "    return pd.DataFrame(data = temp, columns = header)\n",
    "\n",
    "\n",
    "\n",
    "def get_header (soup):\n",
    "    '''\n",
    "    Given a BeautifulSoup object of HTML code, extract table heading information as list of strings\n",
    "    '''\n",
    "    \n",
    "    header = soup.select('thead th')\n",
    "    for i in range(len(header)):\n",
    "        header[i] = header[i].text.strip()\n",
    "\n",
    "    # remove 'Game Stats' data\n",
    "    header = header[:header.index(\"Date\")]\n",
    "    \n",
    "    return ['Name'] + header\n",
    "\n",
    "\n",
    "\n",
    "def get_player (soup):\n",
    "    '''\n",
    "    Given BeautifulSoup object with HTML code, return tuple with player name and role (pitcher/batter) \n",
    "    '''\n",
    "    \n",
    "    t = soup.select('title')[0].text.strip()\n",
    "    \n",
    "    name = t[:t.find(' KBO')]\n",
    "    \n",
    "    if (\"Pitching\" in t):\n",
    "        role = \"Pitcher\"\n",
    "    elif (\"Batting\" in t):\n",
    "        role = \"Batter\"\n",
    "    \n",
    "    return (name, role)\n",
    "\n",
    "\n",
    "\n",
    "def get_website (url):\n",
    "    '''\n",
    "    Given string containing url of website, return BeautifulSoup object with parsed HTML code\n",
    "    '''\n",
    "\n",
    "    # generated from ZenRows.com\n",
    "    apikey = \"b327144420a12f944bbdd4863c9fc0bffb28a1e3\" \n",
    "    params = {\n",
    "        \"url\": url,\n",
    "        \"apikey\": apikey,\n",
    "        \"js_render\": \"true\",\n",
    "        \"premium_proxy\": \"true\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(\"https://api.zenrows.com/v1/\", params=params)\n",
    "    print(response.status_code)\n",
    "\n",
    "    return BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "def fix_team_names (df):\n",
    "    '''\n",
    "    Given dataframe with player information, adjust Team names column to represent the full team name\n",
    "    '''\n",
    "    teams = {'Doosan': 'Doosan Bears', \n",
    "            'Hanwha': 'Hanwha Eagles', \n",
    "            'Kia': 'Kia Tigers', \n",
    "            'Kiwoom': 'Kiwoom Heroes', \n",
    "            'KT': 'KT Wiz', \n",
    "            'LG': 'LG Twins', \n",
    "            'Lotte': 'Lotte Giants', \n",
    "            'NC': 'NC Dinos', \n",
    "            'Samsung': 'Samsung Lions', \n",
    "            'SSG': 'SSG Landers'}\n",
    "    \n",
    "    for i in df.index:\n",
    "        df.loc[i, 'Team'] = teams[df.loc[i, 'Team']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 5): \n",
    "    url = \"https://mykbostats.com/players/\" + str(i)\n",
    "    soup = get_website(url)\n",
    "    \n",
    "    player = get_player(soup)\n",
    "    header = get_header(soup)\n",
    "    rows = soup.select('tbody tr')\n",
    "    data = get_row_data(rows, player)\n",
    "\n",
    "    fix_team_names(data)\n",
    "\n",
    "    if (player[1] == \"Pitcher\"):\n",
    "        data.to_csv('KBO_Pitchers.csv', mode='a')\n",
    "\n",
    "    if (player[1] == \"Batter\"):\n",
    "        data.to_csv('KBO_Batters.csv', mode='a')\n",
    "\n",
    "    sleep(5) #website crawl-delay\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
